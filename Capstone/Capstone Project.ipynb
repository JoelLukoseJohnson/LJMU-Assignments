{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6453fe30",
   "metadata": {},
   "source": [
    "# WEB AND SOCIAL MEDIA ANALYTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1f59b",
   "metadata": {},
   "source": [
    "### MOUNTING GDRIVE WITH COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244bd672",
   "metadata": {},
   "source": [
    "### CHECKING FOR FILES IN DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls drive/'MyDrive'/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0792d7",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab980e1",
   "metadata": {},
   "source": [
    "#### The following data sets are available\n",
    "1. meta data of type .json\n",
    "2. phone data of type .csv\n",
    "\n",
    "The meta data holds information from sellers point of view and the phone data holds user information. We begin by unzipping the meta data file and extracting its content to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5060452",
   "metadata": {},
   "source": [
    "### READING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dad49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the meta data\n",
    "# importing libraries\n",
    "\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "#Path to the meta data zip file 'meta_cell_phones_and_Accessories' is the folder name under 'My Drive'\n",
    "path1 = '/content/drive/MyDrive/meta_Cell_Phones_and_Accessories.json.gz'\n",
    "\n",
    "# Path to meta data .json file\n",
    "path2 = '/content/drive/MyDrive/meta_Cell_Phones_and_Accessories.json'\n",
    "\n",
    "# Unzipping the meta data file\n",
    "with gzip.open(path1, 'rb') as f_in:\n",
    "    with open(path2, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the unzipped meta data into a Python list. The result will be a list of dictionaries. \n",
    "import json\n",
    "\n",
    "# Empty list to store the dictonaries\n",
    "phonemetadata = []\n",
    "\n",
    "# Reading the dictionaries in the json file and appending it to the list phonemetadata[]\n",
    "with open(path2, 'r') as f:\n",
    "    for line in f:\n",
    "        phonemetadata.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e68fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of entries in the phonemetadata list\n",
    "len(phonemetadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b635d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the list phonemetadata into a data frame\n",
    "df_meta=pd.DataFrame(phonemetadata)\n",
    "\n",
    "#creating new .csv file for meta\n",
    "df_meta.to_csv('/content/drive/MyDrive/meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd163f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading meta.csv file\n",
    "df_meta=pd.read_csv('/content/drive/MyDrive/meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the .csv file of the phone data into a dataframe\n",
    "\n",
    "df_phonedata = pd.read_csv('/content/drive/MyDrive/Cell_Phones_and_Accessories_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43159b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phonedata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691ba40",
   "metadata": {},
   "source": [
    "### DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592c2ed0",
   "metadata": {},
   "source": [
    "#### Converting unix review time to date-time format for better observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c626c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming unixReview time to date time format in phone data\n",
    "from datetime import datetime, timedelta\n",
    "df_phonedata['Date&Time'] = df_phonedata['unixReviewTime'].apply(lambda d: (datetime.fromtimestamp(d) - timedelta(hours=2)).strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phonedata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phonedata.info() #checking for dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1bf824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phonedata.isnull().sum() #checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phonedata=df_phonedata[~df_phonedata['style'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phonedata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phonedata.drop(['vote','image','unixReviewTime'], axis = 1, inplace = True) #dropping the columns with more null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phonedata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18066b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phonedata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31679aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phonedata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f2357",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2568ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd69fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.price.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.similar_item.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752aa62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates in ASIN\n",
    "\n",
    "df_meta.drop_duplicates(subset='asin',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58f9df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=df_meta[\"category\"].apply(lambda x:x[31:35]==\"Cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4726c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta=df_meta[mask]\n",
    "df_meta.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d648fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80560ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.drop(['tech1','tech2','fit','date'], axis = 1, inplace = True) #dropping columns with null values in meta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta1=df_meta[~df_meta['price'].isna()] #checking for null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta1=df_meta1[~df_meta1['similar_item'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d63197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta1=df_meta1[~df_meta1['brand'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d80d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta1.to_csv('/content/drive/MyDrive/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta=pd.read_csv('/content/drive/MyDrive/data.csv')\n",
    "df_meta.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aeb507",
   "metadata": {},
   "source": [
    "#### Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f121bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the meta and phonedata datasets\n",
    "\n",
    "reviews = pd.merge(df_phonedata, df_meta, how=\"inner\", on=[\"asin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554913d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv('/content/drive/MyDrive/review.csv') #creating new .csv for merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/content/drive/MyDrive/review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e66135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbfba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_excel(\"/content/drive/MyDrive/for_tableau.xlsx\", engine='xlsxwriter') #saving it as xlsx for tableau analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes the overall category to categorical from numerical for easier visualizations\n",
    "df[\"classes\"] = df[\"overall\"]\n",
    "df[\"classes\"].replace({5.0: \"positive\",4.0:\"positive\",3.0:\"neutral\",2.0:\"negative\",1.0:\"negative\"}, inplace=True)\n",
    "df[\"classes\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ed274",
   "metadata": {},
   "source": [
    "### TEXT ANALYTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading stop words from a text file in to a list\n",
    "stop_words = [line.rstrip('\\n') for line in open('/content/drive/My Drive/stop_words_long.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "  # creates new column with corresponding class labels, the output variable.\n",
    "  df['y'] = df['overall'].apply(classify)\n",
    "\n",
    "  # dropping uneccesary columns for the analysis\n",
    "  df = df.drop(labels=['Unnamed: 0', 'verified','asin' ,'style','reviewerName',  'description','title', 'main_cat' ],axis=1)\n",
    "  \n",
    "  # dropping all NaN values from the column reviewText\n",
    "  df = df.dropna(axis=0, subset=['reviewText'])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf297121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits review rating into different classes, say positive(2), neutral(1) and negative(0). \n",
    "def classify(x):\n",
    "    if x == 5.0 or x==4.0:\n",
    "        return 2\n",
    "    if x==3.0:\n",
    "        return 1 \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ca9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes all the punctuations for the strings of reviewText, ie '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "def remove_punctuation(text):\n",
    "    no_punct=[words for words in text if words not in string.punctuation]\n",
    "    words_wo_punct=''.join(no_punct)\n",
    "    return words_wo_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf9e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(df):\n",
    "  # Removing all the punctuations from the words, and changing the words to lower case to maintain uniformity\n",
    "  df['reviewText']=df['reviewText'].apply(lambda x: remove_punctuation(x.lower()))\n",
    "  # stemming\n",
    "  stemmer = PorterStemmer()\n",
    "  # stop words are the words like \"the, I, our etc\"\n",
    "  words = stopwords.words(\"english\") \n",
    "  df['cleaned_reviews'] = df['reviewText'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6939ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewText']=df['reviewText'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a51fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=text_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ba688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping null values to have a cleaner dataset\n",
    "\n",
    "df = df.dropna(subset=['cleaned_reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"/content/drive/MyDrive/final_tab.xlsx\", engine='xlsxwriter') #xslx file for final tableau analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59776c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f763e1b5",
   "metadata": {},
   "source": [
    "#### Distribution of sentiment analysis across the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary visualisation libraries\n",
    "\n",
    "import torch #the pytorch library, used for modeling and formatting our data to be compatible in a pytorch environment\n",
    "import pandas as pd #for dataframe reading, cleaning functions\n",
    "from tqdm.notebook import tqdm #used as a progress bar\n",
    "\n",
    "# Importing necessary libraries for basic visualization as well as word clouds\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Using textblob to generate sentiment for easy visualization\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af56d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of sentiment across the dataset\n",
    "# Distribution of sentiment class\n",
    "fig, axes = plt.subplots(1, figsize=(8,5))\n",
    "fig.suptitle(\"Ratio of Neutral Negative and Positive ratings\", fontsize = 20)\n",
    "plt.tight_layout(pad = 3.5)\n",
    "sns.countplot(x = \"classes\", data = df)\n",
    "axes.set_xlabel(\"Review_sentiment\", fontsize = 20)\n",
    "axes.set_ylabel(\"Count\", fontsize = 20)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of neutral, negative, positive words in train and test data\n",
    "def pert_count(data, category):\n",
    "    return (len(data[data[\"classes\"] == category])/len(data)) * 100\n",
    "print(f\"Percentage of neutral words in train --> {pert_count(df, 'neutral')} %\")\n",
    "print(f\"Percentage of negative words in train --> {pert_count(df, 'negative')} %\")\n",
    "print(f\"Percentage of positive words in train --> {pert_count(df, 'positive')} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92b59e3",
   "metadata": {},
   "source": [
    "#### Words and word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most repeated words in reviews\n",
    "stopwords = set(STOPWORDS)\n",
    "def word_cloud(data, title):\n",
    "    wordcloud = WordCloud(\n",
    "    background_color = \"white\",\n",
    "    max_font_size = 40,\n",
    "    max_words = 200,\n",
    "    stopwords = stopwords,\n",
    "    scale = 3).generate(str(data))\n",
    "    fig = plt.figure(figsize = (8.5, 8.5))\n",
    "    plt.axis(\"on\")\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=15)\n",
    "        fig.subplots_adjust(top=2.35)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ad541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating all records into a different variable to generate wordcloud based on category\n",
    "neu=df[df[\"classes\"] == \"neutral\"][\"cleaned_reviews\"]\n",
    "neg=df[df[\"classes\"] == \"negative\"][\"cleaned_reviews\"]\n",
    "pos=df[df[\"classes\"] == \"positive\"][\"cleaned_reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc04ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating wordcloud based on category\n",
    "word_cloud(neu, \"Most Repeated words in neutral reviews\")\n",
    "word_cloud(pos, \"Most Repeated words in positive reviews\")\n",
    "word_cloud(neg, \"Most Repeated words in negative reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9677b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for bigram\n",
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2),stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]# apply function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc99be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for bigram plots\n",
    "def n_gram_plot(data,title,color):\n",
    "    x=[x[0] for x in data]\n",
    "    y=[x[1] for x in data]\n",
    "    sns.barplot(y,x,color='{}'.format(color))\n",
    "    plt.title('{} Reviews Bigrams'.format(title),fontsize=15)\n",
    "    plt.yticks(rotation=0,fontsize=15)\n",
    "\n",
    "common_words_good = get_top_n_bigram(pos, 5)\n",
    "common_words_neutral= get_top_n_bigram(neu, 5)\n",
    "common_words_bad= get_top_n_bigram(neg, 5)\n",
    "\n",
    "# bigram plot using function above\n",
    "plt.figure(figsize=(10,5))\n",
    "# good reviews bigrams\n",
    "plt.subplot(151)\n",
    "n_gram_plot(common_words_good,'Good','green')\n",
    "#============================================= \n",
    "#neutral reviews bigrams\n",
    "plt.subplot(153)\n",
    "n_gram_plot(common_words_neutral,'Neutral','orange')\n",
    "#============================================= \n",
    "#bad reviews bigrams\n",
    "plt.subplot(155)\n",
    "n_gram_plot(common_words_bad,'Bad','red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating sentiment polarity using external libraries\n",
    "df['sentiment'] = df['cleaned_reviews'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5842467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barchart using to visualize the sentiment distribution generated above\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.margins(0.02)\n",
    "plt.xlabel('Sentiment', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel('Frequency', fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.hist(df['sentiment'], bins=55)\n",
    "plt.title('Sentiment Distribution', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479c577",
   "metadata": {},
   "source": [
    "We can conclude that there's more reviews that look neutral exist in our dataset than there are positive or negative reviews. We also see that people tend to put in reviews only when its positive or when its neutral and they barely bother to put up a review when its a really bad product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f475bc5f",
   "metadata": {},
   "source": [
    "##### Correlation heatmap to analyse the dependence between review length and sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310fdbd5",
   "metadata": {},
   "source": [
    "It is a known fact that individuals tend to rant more about a product than they would praise a product. The following correlation heatmap further proves that face, there's a negative correlation between review length and the sendiment indicating negative reviews tend to be longer than positive or neutral ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap from overall rating, review length, sentiment and word count\n",
    "\n",
    "# calculating word count and review length\n",
    "df['word_count'] = df[\"reviewText\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "df['review_len'] =df[\"reviewText\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "# visualising the heatmap\n",
    "correlation = df[['overall','sentiment', 'review_len', 'word_count']].corr()\n",
    "mask = np.zeros_like(correlation, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "sns.heatmap(correlation, cmap='YlGnBu', annot=True, annot_kws={\"size\": 12}, linewidths=10, vmin=-1.5, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for Tokenization\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "#Libraries to implement Naive Bayes \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "#Libraries to implement and optimize Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#To create Pipelines and select best feautures before passing through the model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "#Pre-processing and NLP libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download(\"stopwords\")\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "#Visualization tools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization methods to break down text into tokens(words)\n",
    "#Using TF-IDF with ngram range of (1,1)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df= 3, sublinear_tf=True, norm='l2', ngram_range=(1,1))\n",
    "#Using Bag of Words with ngram range of (1,1)\n",
    "counter_vectorizer = CountVectorizer(ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d5c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating test-train with standard 80-20 split\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]\n",
    "X_train_class = df_train[\"cleaned_reviews\"]\n",
    "Y_train_class = df_train[\"y\"]\n",
    "X_test_class = df_test[\"cleaned_reviews\"]\n",
    "Y_test_class = df_test[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d732cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Model Pipelines for various models with the pattern [Tokenizer Method][SelectKBest features][Model+Paramaters]\n",
    "text_clf_classifier1 = Pipeline([('vect', counter_vectorizer),('chi',  SelectKBest(chi2, k=1200)),('clf', RandomForestClassifier()),])\n",
    "#text_clf_classifier1 = Pipeline([('vect', counter_vectorizer),('chi',  SelectKBest(chi2, k=1200)),('clf', RandomForestClassifier(criterion='entropy')),])\n",
    "text_clf_classifier2 = Pipeline([('vect', counter_vectorizer),('chi',  SelectKBest(chi2, k=2890)),('clf', MultinomialNB(alpha=11)),])\n",
    "text_clf_classifier4 = Pipeline([('vect', counter_vectorizer),('chi',  SelectKBest(chi2, k=2890)),('clf', BernoulliNB(alpha=11)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2224b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to execute and run Model Pipelines and display Model Paramaters used along with Accuracy\n",
    "def run_model(text_clf_classifier):\n",
    "  print(text_clf_classifier)\n",
    "  classifier_model = text_clf_classifier.fit(X_train_class,Y_train_class)\n",
    "  print(np.mean(classifier_model.predict(X_test_class)== Y_test_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad467cb",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier Parameters tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9888564",
   "metadata": {},
   "source": [
    "1. No of estimators - No. of trees - 100,\n",
    "2. Information Gain criteria - Entropy & Gini Index : Gini Index gives better accuracy.\n",
    "3. Cost complexity pruning paramter - 0.0(No pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a7744",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Classifier - Count Vectorizer:\")\n",
    "run_model(text_clf_classifier1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4d86e",
   "metadata": {},
   "source": [
    "#### Naive Bayes methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f5166",
   "metadata": {},
   "source": [
    "1. Learning Rate Variants : 0.0001,0.001,0.01,0.1,1,3,5,8,11,13,15\n",
    "2. Best Results were given by alpha=11 for Counter Vectorizer for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ccae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running Multinomial Naive Bayes - Counter Vertorizer for variety of learning rates to best estimate hyperparameter value\n",
    "x_alpha=[0.0001,0.001,0.01,0.1,1,3,5,8,11,13,15]\n",
    "y_accuracy=[]\n",
    "for x in x_alpha:\n",
    "  clf = Pipeline([('vect', counter_vectorizer),('chi',  SelectKBest(chi2, k=2890)),('clf', MultinomialNB(alpha=x)),])\n",
    "  classifier_model = clf.fit(X_train_class,Y_train_class)\n",
    "  y_accuracy.append(np.mean(classifier_model.predict(X_test_class)== Y_test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0483fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_alpha,y_accuracy,'^k')\n",
    "plt.ylabel('Accuracies')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.show()\n",
    "print(y_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4713e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multinomial Naive Bayes - Count Vectorizer:\")\n",
    "run_model(text_clf_classifier2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c17ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bernoulli Naive Bayes - Count Vectorizer:\")\n",
    "run_model(text_clf_classifier4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76efbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(review):\n",
    "  prediction_df = pd.DataFrame([[review,0]],columns = [\"reviewText\",\"y\"])\n",
    "  prediction_df = text_process(prediction_df)\n",
    "  classifier_prediction = classifier_model.predict(prediction_df[\"cleaned_reviews\"])\n",
    "  return classifier_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction(\"works good, viewing angle critical for good picture,the size was perfect for the location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5415b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/content/drive/MyDrive/final_result_for_capstone.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
